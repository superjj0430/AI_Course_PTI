CNN尚未補完

RNN

1.
接下來是循環神經網路RNN，這邊簡單講義下為什麼會什麼他會出現，傳統的神經網路他的輸出與輸入都是互相獨立的，也就是說假如現在我現在要做圖片辨識，我每一張圖片通過模型的到的結果，與其他輸入資料無關，我丟進去一張貓的圖片，其他憶起丟進去的圖片不管是貓是狗是牛，都部會影響我的輸出，
但假如我們現在要處理的任務，他每個輸入都與其他輸入有一些關西，那用傳統神經網路的效果就部會那麼理想，最常見的例子就是和語言相關的任務，下面有個簡單的例子，NLP中文叫自然語言處理，也就是比較常聽到的文字探勘，現在我們要去辨識這兩句話中KEEP代表的意思，可以發現雖然兩句話的KEEP寫法都異樣，但根據其他單詞的詞性，造成一個代表動詞的存放，一個代表名詞的生活費，這也就是剛剛所說的，當我們輸入與其他輸入有關時，我們用傳統神經網路將牠們分開來獨力辨識會有無法判斷的問題


2.
這邊是比較常見的RNN架構圖，架構看起來挺簡單的，而讓RNN能處理序列資料的主要差異就是他加入了順序的概念，圖上的輸入X與輸出H的旁邊都有寫個T，T代表當前的時間，T-1代表前一個T+1代表下一個


3.
這邊再將這個結構用更簡單的圖來看，這邊先把旁邊的W遮住，可以發現RNN也是由輸入層、隱藏層和輸出層構成的，像X代表輸入，經過權重U的計算到達隱藏層S，再經過一次變成輸出，








4.
現在來了解意下剛剛遮住的W是甚麼，W叫做循環層，剛剛有說道RNN的每個輸出不完全取決於當前輸入，而是也與其他輸入有關，這邊我們將左邊的圖案照時間線展開就比較清楚了，當我們的網路在T時刻接收到輸入XT時，隱藏層的值是ST，輸出值是OT，關鍵的一點是ST的值不僅僅取決於XT，還取決於ST-1，簡單來講就是我們在計算當前T時刻的輸出時，會用到當前ST的參數與前一時刻ST-1的參數，因此RNN就可以用這種方式將不同時間的輸入產生關連，上面是計算公式，也可以看到計算當前ST時有用到ST-1


5.
接下來有一個動圖幫大家更好理解RNN執行的時候是怎樣WORK的，首先是有個當前輸入XT，和前一個隱藏層參數HT-1，接著會把隱藏層參數HT-1與HT合併計算，計算完後會產生一個輸出，並且模型會把當前隱藏層參數傳遞到下一個模塊繼續進行計算，整體RNN的理論大概就是這樣


LSTM

1.
接下來是RNN的其中一種類型LSTM，剛剛在介紹RNN架構時可以發現，他所使用到的隱藏層資料只有當前輸入的前一個隱藏層，當我們資料很長一串時會產生一個問題，當我們的隱藏層參數一直不段傳遞下去時，越前面的參數所造成的影響會越來越小，也就是模型把前面的參數弄丟的感覺，在學術上叫做長距離依賴問題，而LSTM主要就是要解決這個問題，他跟RNN比起來多了一些不同的東西


2.
這邊是LSTM的架構，，可以發現LSTM和RNN異樣神经网络结构都是由完全相同结构的模块进行复制而成的，但他比RNN多了一些不同的神經網路層，並且透過一些簡單的數學計算來解決長距離依賴的問題，

在將她拆開來說明前先說明一下圖中所使用的圖標，綠色大框代表細胞狀態，黃色方塊是神經網路層，粉紅原點是數學運算，黑色箭頭代表向量的傳遞友就是參數，合併的箭頭代表參數的合併，分開代表複製


3.
首先是叫做遺忘門的東西，跟名字異樣他就是要用來忘掉某些東西，当输入新的信息时，模型若需遗忘旧的信息，此时通过遗忘门来完成。遗忘门是LSTM单元的关键组成部分，可以控制哪些信息要保留、哪些信息要遗忘，并且避免梯度随时间反向传播时引发的梯度消失和梯度爆炸的问题。

遗忘门决定LSTM从上一时刻的细胞状态 Ct-1 中丢弃什么信息。该门读取 前一層隱藏層參數ht-1 和 輸入xt ，然后通过sigmoid将其映射到0到1之间的数值，最终该数值再与细胞状态Ct-1相乘，来决定Ct-1中该丢弃什么信息。当该数值为1时表示完全地保留 Ct-1的信息，当该数值为0时表示完全地丢弃 Ct-1 的信息


4.
接著是輸入門，功能是确定哪些新的信息被保留在细胞状态中ct中，

输入门用于控制网络当前输入数据xt 流入记忆单元的多少，即有多少输入信息可以保存到 ct 中。输入门包括两部分，第一部分：由sigmoid组成的“输入门”产生的介于0到1之间的控制信号 it ，用来控制 ct加帽子 输入的程度；第二部分：通过一个tanh层产生当前时刻的候选细胞状态 ct加帽子，这个值将由 it 决定添加到细胞状态中的程度


5.
接著就是要更新當前細胞狀態，将旧的细胞状态 ct-1 更新为当前细胞状态 ct 。有了遗忘门产生的控制信号ft ，tanh层产生的候选细胞状态 ct加帽子 ，输入门产生的控制信号 it ，就可以将 ct-1更新为 ct 。首先 ft乘上ct-1 确定上一个细胞状态要保留的信息；然后it乘上ct加帽子得到候选细胞状态需要保留的信息；最后将这两部分相加，得到最终的当前时刻的细胞状态ct，到這邊就完成細胞狀態的更新了








6.
最後就是輸出門的結構，输出值基于细胞状态，但是会有一个过滤的过程，这里也包括两部分操作：第一部分，由sigmoid组成的“输出门”产生的介于0到1之间的控制信号 ot ；第二部分，将最终产生的输出信息 經過tanh計算的ct 与控制信号 ot相乘，得到最终的输出值 ht 。输出门控制记忆单元ct 对当前输出值 ht 的影响，也就是CT中的哪一部分会在时间T下的 输出

最後和RNN異樣會將資料傳遞到下一個時刻T+1，除了将包含长期信息的细胞状态 CT 传递到下一时刻，也将当前时刻的输出HT作为近期信息传递到下一时刻，到這邊LSTM的工作原理就講完了，後面異樣有一個執行過程的動圖


7.
首先異樣有個當前輸入XT，和前一個隱藏層參數HT-1，接著是遺忘門的計算後更新細胞狀態CT-1，接著是輸入們的計算將CT-1更新乘CT，然後透過輸出們計算決定CT中那些資料會繼續傳遞下去，接著將細胞狀態CT和隱藏層參數HT傳遞到下一個LSTM模塊進行計算
 
深度學習概論

1.
接下來是一些神經網路的基礎，這邊會先將整個神經網路拆成幾個小部份去做說明，最後在合併起來看整個架構時就會比較好去理解了

首先是線性函數或也可以叫做得分函數，假如現在做圖片分類的任務，輸入一隻貓，接下來要得到他在每個類別的一個得分，比如說屬於狗的得分屬於貓的得分屬於牛的得分

接著把這張圖在分細一點，這張圖總共有有32*32*3 3072個像速點，每個像速點對結果的影響的重要性也不太相同，比如分辨一隻貓，貓眼睛的資料和後面背景的資料重要性部會相同，而這邊所說的重要性也就是權重參數W，這邊有3072個像速點，每個像速點都會對應一個權重值，也就是會有3072個權重


2.(加矩陣式意圖)(加投影片)
接著看算式，首先看藍字輸入X，神經網路計算方式大家可以這樣理解，這邊我們將貓的每個像速點拉長，因此會產生一個3072*1的矩陣，然後還有剛剛提到的權重參數，這裡有3072個像速點，權重矩陣就會變成1*3072的矩陣，接著輸入矩陣與權重矩陣相成，依照矩陣乘法的規則最終會得到一個1*1的矩陣也就是一個得分值，

這邊假設我們要做一個十分類的任務，也就是要判斷圖片是屬於十個類別中哪一個類別，對於每個像速點我們都要去計算他在這十個類別中各自的得分，最後在去評估所有像速點的結果，而對於權重W來說，有十個類別代表會有十組不同的W，也就是貓的類別有自己的權重狗的類別也有自己的權重，

因此這邊看到上面的這個紅字算式，3072代表的是每個類別有3072個權重值，而10代表的是有十個類別，然後綠色的部分因為有十個類別，我們最終獲得的值也會有十個，也就是這張圖片在這十個類別中各自的得分，這邊就是得分函數在數學上的表示，

最後看到紫色的地方叫做偏置項b，在神經網路中權重w通常是用來對結果有一個決定性的作用，而偏置項b則是做一個微調的動作，微調的意義是讓我們的神經網路在擬合上更加的靈活，而偏置項10*1的意思是每個類別都會有各自不同的偏置項，因此這邊有十個類別就會有十個偏置的意思

3.(加投影片)
然後這邊就是剛剛的計算方式，現在假設只將這張圖片分成只有四個像素點燃後不考慮它圖片是彩色還黑白的，像速點就是56、231這排，然後做一個三分類任務，類別分別是貓狗和船，對於這三個類別來說就會有三組權重W，這邊權重矩陣是一個3*4的矩陣，3代表有三個類別四代表有四個像速點，計算方式就是權重矩陣與輸入矩陣相乘，最後再加上偏置項B，就會得到這張圖片在每個類別的得分值了，然後這邊看一下權重矩陣裡面的值，可以發現數值有大有小，而這邊的大小也就是不同像速點對於分類類別的重要性，重要性越大值會越大，
然後說一下權重值是哪來的，通常初始值都是隨機值，在計算的過程中會透過之前說過的梯度下降的方式去做優化，這邊可以看到分類結果的部分將貓分成了狗，也就代表這個神經網路還需要繼續優化


4.(加投影片)
然後是決策邊界，這邊簡單了解一下就可以了，權重參數是主要控制決策邊界的走勢，偏置參數是用來上下左右微調的，這邊大概看一下決策邊界會長怎樣就行了


5.(加投影片)
接著是如何衡量結果，神經網路不管是分類任務還是回歸任務都是可以處理的，差別在於損失函數如何定義，神經網路的架構通常部會取改變，損失函數不同可以處理的任務也不同，


6.
這邊損失函數複習一下，損失函數簡單來說就是數據的損失也就是誤差，加上一個懲罰項防止過擬合，最後一句話跟大家說明一下什麼意思，神經網路的優點是能解決的問題比傳統機器學習多，但他也有一個很大的缺點就是神經網路的效能過於強大，越強大的模型或算法他過擬合的風險就越高，所以說在實際在訓練神經網路的時候，我們很常不是因為他效能不好去修改他，而是因為細能太好才去修改他，因為一個學習的太好過擬合的模型是不能拿來使用的





7.
接下來是SOFTMAX函數，這邊簡單回想一下剛剛的分類任務，我們神經網路計算出的結果都是一個得分值，但在實際上我們比較常會說的是這張圖片是貓的概率是多少是狗的概率是多少，這邊大家如果還記得的話就跟我們之前說邏輯回歸模型很像，邏輯回歸是透過SIGMOID函數將分類結果映射成概率，而SOFTMAX的功能也是差不多的意思


8.
到這邊簡單REVIEW一下剛剛講的東西，首先會有一個藍色框的得分函數，輸入X配上權重W會得到一個得分值，有了得分值後，可以基於得分值和真實值去計算紅色框的損失函數，在計算完損失函數後加上綠色框的懲罰像會得到損失值，然後如果是分類任務會多一個SOFTMAX去轉換得分值變成概率，回歸任務的化部用轉換，到計算完損失值這邊就叫神經網路的向前傳播，

而到這個步驟後接下來要做什麼，剛剛有說道神經網路會去更新權重，也就是找到最適合的權重，所以接下來要做的就是去優化權重降低我們的損失值，在神經網路裡這部分叫做反向傳播

這邊在跟大家說明一個點，剛剛用的例子都是為了方便理解簡化過的，像是一個輸入X經過一組權重W計算就得到結果，但在實際上，權重部會只有一組，輸入X可能會經過很多次不同權重的計算才會得出一個結果值，可以理解成輸入成完第一組權重W1後，可能還會呈上第二組權重W2一值成到第N組WN，也就是說我們得分函數會經過多次的轉換才會得到結果值，這點也是跟傳統機器學習不太異樣的地方，神經網路在得分函數計算的部分複雜化了很多，將原本只有一組的權重變成多組去計算


9.
接下來是是梯度下降，神經網路的反向傳播也是用梯度下降的方式去做計算，這邊之前有講過這邊就不再重複，投影片上的內容大家就當作複習快速看一下就可以了 


10.
這邊是梯度下降的類別


11. 
然後這邊講一個反向傳播的小例子，假設現在有XYZ三個輸入，最終結果會是FXYZ函數，他等於X+Y成Z，這邊FXYZ函數大家可以當作損失函數去看，現在我們將FXYZ函數拆開來，F可以看成Q成上Z籃框這便，而Q又等於X加上Y宏框這邊，接下來就是之前說過的透過求偏導找最小值，這邊分別要求XYZ三者的偏導，計算的順序從後往前算，首先先求F對Q跟Z的偏導，接著再求Q對XY的偏導，這邊也就是說明神經網路在反向傳播的過程中，是由後往前逐層去計算的


12.
這種傳播方式有個名字叫做練式法則，這邊意思就是梯度在神經網路裡面傳遞的方式是一步一步船的，大概就是這個意思，旁邊圖片就是剛剛計算過程的是意圖


13.
到這邊神經網路比較重要的細節大概都講過了，也就是向前傳播和反向傳播，接下來就是把剛剛講的東西合併起來去了解，組成一個完整的神經網路結構，

這邊看圖，左邊的是生物的神經網路，右邊是一個數學公式版的神經網路，這邊就是給大家看一下神經網路為什麼叫神經網路，因為他的出發點就是去模擬生物體內資訊透過神經網路的傳遞方式這個樣子















14.
然後這邊是神經網路的架構圖，這邊跟大家解釋神經網路是怎麼去工作的，首先是他有一個層次的結構，圖上可以看到他有個輸入層、有一個隱藏層1、隱藏層2和一個輸出層，這邊其實就是前面講的，輸入可能會經過多次轉換才會得到結果，也就是一層接著一層計算下去的，這就是層次結構

然後神經元，簡單來說就是我們的參數，向前面的例子，我們輸入3072個像速點，也就是在輸入層這邊我們會有3072個神經元

然後看到隱藏層1的地方，可以看到輸入層和隱藏層1的地方連接許多線，輸入層的每個神經元都和隱藏層1的美個神經元連接，這個結構就叫全連接，

而隱藏層中間四個神經元給大家說明一下，簡單來說就是神經網路為了能更好的去學習資料而將原始的特徵數量變多，他的做法舉裡來說可能是特徵1成0.1特徵2成0.2特徵3成0.3相加來的，而這特徵對神經網路來說可能比原始特徵更有意義，但對人類來說就沒辦法去對這些特徵去做一個具體的解釋，這也就是大家詬病的神經網路是黑盒子問題，神經網路這些特徵怎麼來的不知道、代表的意義也不知道，單純就是數學計算轉來的東西，

接著回來說明這些線是什麼，這先線就是一直提到的全重，也就是資料經過各種全重計算去進行一系列的轉換，
然後最後一點非線性，在每個矩陣計算結束後要傳到下一層時，都會先經過一個非線性函數去轉換


15.
整體的計算方是就像這樣，這邊非線性函數用MAX函數來指定，MAX函數裡可以看到全重成上輸入，經過MAX的非線性轉換後才會成上下一個全重W2，如果在堆疊一層就會長的向下面異樣，

總結來說，神經網路為什麼這麼強大，也就是透過一堆網路層和各種數學計算去堆疊他的參數輛，通常參數的數量都會是百萬級別以上的，

但參數的數量不是說越多越好，通常參數或者說神經元越多，在訓練及的表現都會越好，但同時也越容易過擬合，而且參數越多也代表計算量越大，因此模型執行的速度也就會越久

另外就是當我們每增加一個神經元，對神經網路來說她不只是增加一個而已，而是增加一組神經元，因為美個神經元都會經過一系列的計算，因此假如我們增加一個神經元，他最終增加的參數數量可能是多了一百萬個這麼多，這也是神經網路的一個特色


16.(拿掉非線性第二夜)
接下來看正則化也就是懲罰項對結果的影響，懲罰項越小越符合訓練及的結果，圖片上可以看到懲罰值越小越能擬和訓練及，但在測試集也就是實際結果上的表現則不意定會更好，也就是過擬合的問題


17.
然後是參數數量對結果的影響，參數越多越能擬和訓練及，但同樣在實際結果上的表現則不意定會更好


18.
接下來是激活函數，也就是前面提到的非線性轉換，這邊就舉三個比較常用的，首先是之前提過的SIGMOID，轉換完後在01之間，優點是算法比較簡單，缺點就是要算很久可能改變數居分布，還有一個神經網路裡很常會聽到的詞梯度消失，簡單來說就是當我們的參數經過轉換後越接近於0，他之後向後傳播的力度會越來越小，到最後會小到無法更新參數的問題，

跟他相對的是梯度爆炸，也就是參數越更新越大，大到神經網路無法計算的程度


19.
在來是TANH函數，轉換後在-11之間，優點是比起SIGMOID部會去改變數據分布，缺點是計算方式是冪運算，異樣不好算，而且還是有梯度消失的問題


20.
最後是RELU，這個算是裡面最常用的激活函數，他也可以叫線醒整流函數，特點就是運算速度很快，沒有梯度消失問題，但因為取值大小無上限而可能有梯度爆炸的問題，另外還有一個RELU DEAD問題，意思是當RELU轉換的值小於0，他會直接將他變成0，會導致梯度值皆消失無法反向傳播

21.(補投影片)
然後是資料前處理，這邊之前的課也講過了，標準化跟歸一化，這邊也不多做說明


22.(補投影片)
然後是參數初始化，通常一開始都是用隨機的給他選，這邊有RADOM前成一個0.01的意思是讓隨機選的參數差一步要太大，大概就這樣


23.
然後最後一部份，DROPOUT機制，神經網路最大的問題就是太複雜太容易過擬合，前面說可以加正則化處理，另一種方法就是用DROPOUT處理，透過削減神經網路網路層的威力來處理，左邊可以看到是原本的架構，可以發現全連結，美個神經元都相連著，而右邊的圖可以發現線就沒這麼多了，感覺稀疏了一點，而右邊就是DROPOUT的結果，它的作用是在訓練的過程中，在每一層隨機殺死一些神經元，殺死的意思就是在這次的反向傳播更新全重的時候，不更新到殺死的神經元，以降低神經網路擬合的能力

到這邊基礎就差不多講完了
