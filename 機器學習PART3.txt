4.
決策樹在平常生活中很常見，這邊有個相親的例子，下面是小美跟媒婆之間發生的對話，小美分別問了媒婆對方的幾個條件，對方都符合後才與對方約見面，而小美做決定的方式，就是基於決策樹做的決定，下面已經把樹畫出來了，這就是一個簡單的決策樹了


5.
他的構造和工作原理也很簡單，決策樹由內部節點和葉子組成，內部節點表示特徵，也就是剛剛帥不帥的問題，葉子就是最後決定約還是不約，然後她的棕作方式就是對某個特正測試，根據結果再將樣本分配下去直到樣本達到葉子節點

6.
決策樹在構造的過程中主要分成三個部分，首先特徵選擇問題，通常我們希望找到的特徵有較高的區分度

再來就是如何生成，也就是用哪個算法生成
最後是當我們的模型結果過擬合時，在決策樹裡的剪枝手法能補救’


7.
這邊用一個打籃球的例子舉例怎麼去構造一棵樹，資料部分有四個特徵，分別是天氣溫度濕度跟有沒有颳風，最後是要預測有沒有去打籃球


8.
首先是特徵選擇問題，這邊有幾個簡單的概念了解後就知道怎麼選了，首先是純度，純度簡單來說就是讓我們目標變量分歧程度最小，這邊直接看例子，現在有三個集合，集合一是六次都去打球，集合二是指打四次，集合三則是打三次，按照樣本的分歧程度排序就是集合一純度最大，因為六次都異樣去打球，純度最小則是集合三，打球沒打球佔各伴






9.
再來是信息商，商如果大家還記得的畫上次有講道商是混亂程度，而這邊的信息商表示了一組樣本的混亂程度，樣本越混亂越難做出決定，下面是他計算公式，當樣本不確定性越高，信息商也就越高


10.
這邊也直接看例子，現在有兩集合，集合一是打五次求一次沒打，集合二則是打球沒打球個半，這邊直接套剛剛公式就能算除來了，算完集合A的信息商為0.65，


11.
接著看集合B的信息商為1，


12.
從上面的計算結果可以發現，當我們信息商越大，純度也就越低，而當我們集合中的樣本像B異樣均勻混和時，信息商最大純度最低，這個時候也越難做出決定


所以這時候我們會拿特徵對樣本進行劃分，讓純度變高、商變小，也就是說做了分類這件事，就像下面例子異樣，原始資料是純度最低的情況，用是否刮風進行分類後，可以看到純度變高了
而決策樹選擇樣本的方式就像這樣，當我給定一個條件，如果能使我樣本純度增加最高，也就更利於模型做出選擇，因此會選擇使純度增加最多的分類特徵當作節點，


13.
因此為了比較哪個特徵增加了最多的純度，這邊可以用信息增益的方式，信息增益指的是當得知特徵X的信息後，使類別Y信息不確定性減少的程度，下面是計算公式，簡單來說就是劃分前資料的信息商與劃分後資料的信息商的差

14.
這邊異樣用前面打籃球的例子




15.
首先計算原始資料的信息商為0.985，接下來分別計算每個特徵的條件商，也就是分類後的混亂程度，這邊用晴天陰天小雨三種天氣來做劃分，上面數字代表在資料及中的第幾筆資料，分類完後就可以算各類別的傷了


16.
接著就可以開始算用天氣來劃分的信息增益，首先是原始資料的商，然後有剛剛分類後計算的商，接著將他們相減就可以了

用其他特徵做劃分的信息增益的計算過程這邊就略過，直接看結果，可以看到溫度的信息增益最大，因此先以這個為節點劃分樣本，接下來就是重複找信息增益最大的特徵直到分類完成，最後會長這樣


17.
接下來是生成的算法，這邊的算法指的是度量純度的指標，跟剛剛信息增益概念都很像，只是數學算法不太異樣，這邊有個比較的表格，目前較常用的為CART算法，他可以做分類也可以回歸


18.
最後是剪枝，剪枝的意思就是將不需要的枝幹節點給拿掉，會需要剪枝的原因很大部分是因為之前提過的過擬合，而剪枝策略分成兩種，第一種預剪枝是在模型還在進行構造時，若某個節點無法獲得準確度的提升，將會直接把該節點當作最終的葉節點

另一種後剪枝則是等決策樹構造完後，由底層的葉節點往上評估，若某個節點無法提升準確率，就會用這個節點子樹的葉子節點来替代該節點

 
19.
接下來還是跟跟決策數有關的模型隨機森林，他的基本原理就是結合多顆CART決策樹，並加入隨機分配的訓練資料，來增強訓練結果

這種結合多個弱學習器來構建一個強學習器的方法叫ENSEMBLE，隨機森林的架構圖長這樣，就是剛剛許多決策樹合起來


20.
通常我們進行任務時只會有一個資料集，所以需要行程多顆具有差異性的決策樹進行ENSEMBLE才有意義，因此就要用原本的資料集產生出不同的小資料集，作法有兩種，首先是bagging法，全文是bootstrap aggregation，這種方法會從訓練資料取部分資料訓練，取出的部份資料使用完會放回去，因此這方法產生的決策樹可能會有部分資料重複，但大致上樣本還是會不同，因此決策數之間是有差異性的，最終會用投票的方式在所有決策樹中得出結果

然後是boosting法，剛剛的bagging有點像電池並聯的感覺，而boosting則是串連，boosting法主要就是將舊分類器分類錯誤的資料全重提高，加強對錯誤部分的訓練，因此新訓練氣會學習到分類錯誤資料的特性，有點像以前期末考時將之前寫錯的題目再練習一次的感覺

而隨機森林是使用bagging加上決策數來構成的


21.
構建步驟難的部分決策數那邊講完了，首先是bagging法取一些資料出來訓練決策樹，訓練完放回，重複這兩個步驟到森林完成，最後再以多數決方式分類


22.
分類的模型講完後，這邊是評估我們分類模型的一些指標，通常我們會在分類預測結束後，製作混淆矩陣，矩陣中包含了所有分類出的結果，像是tp就是預測是yes實際上也是yes也就是將正樣本預測為正的意思，

這邊舉個栗子，這個圖順序跟剛剛矩陣有點差異，大家主要看上面的字就可以了，假如醫生將一個男的判斷為懷孕就是預測yes實際為no的，也就是fp false postive，其他以次類推

23.
接著我們可以利用這些tp fp之類的資料來算我們的指標，首先是最常見的準確率，算法就所有預測隊除上所有樣本，這指標有個比較大的問題，就是遇到樣本不平衡時無法有效地評估模型，舉例來說，現在要做癌症檢查，假如現在有99個正常人與一個癌症患者，模型預測結果為全部人都正常，因此他的準確率會有99%，雖然準確率高但意義不大，因為我們主要還是要找出得癌症的那個仁


24.
因此我們除了準確率還有其他衡量指標對應不同情況，首先是recall召回率，他是用還衡量一個分類器挑出我們要的東西占所有我們要的東西的比例，另一個則是precesion精確率，用來衡量分類器挑出的東西中，是我們要的東西的比例


25.
這邊直接看例子比較好懂，首先我們資料有10000個人裡面有十個有病，分類器1檢測50個人其中10個真有，recall算的是全部資料裡時個有病的我找出幾個，因此這邊找出十個也就是100%，而precision算的是檢測的人裡有幾個是有病的，因此50個人裡只有10個也就是20%，其他就以此類推了


26.
最後一個指標是f1 score，他是剛剛的recall和precision的調和平均數，也就是在剛剛兩個指標間取一個較好的平均值，舉例來說的話就像警察抓犯人，警察不希望漏掉任何一個犯人有就是我們recall值要高，另外警察也不希望抓回來的人有好人也就是要precision高，所以需要同時權衡recall和precision兩者
