Agenda

今天的內容主要會講機器學習有哪些類別，分類問題回歸問題是什麼，還有一些比較經典的模型

4.
機器學習依照學習的方式可以分成監督式、非監督式和強化學習，而依照處理的問題類別可分為回歸問題和分類問題兩種

5.
這邊先講監督式與非監督式學習，當我們使用的訓練資料是有標記答案的時候，像是分類貓狗的圖片時，已經事先知道哪些是貓哪些是狗，我們會稱為監督式學習

非監督式則相反，我們訓練資料沒有標記答案只有特徵，如果用剛剛例子來說，模型只能在一群資料裡分出兩個群體，但無法得知哪個群體式貓還是狗

這邊是示意圖，監督式就像圖片上一樣，已經知道了他式圈還是差，而非間度式則是不知道

6.
接下來是強化學習，強化學習是讓我們的模型在環境裡學習，模型的每個行動會有各自對應的獎勵，模型會以獲得最大效益為目的去學習

這邊舉個例子，想像模型是個小朋友，現在他第一次看到火，然後他走到了火的旁邊感受到溫暖，因此認為火是好東西加一分，然後他伸手去摸火，被燙到很痛減一分，因此模型得出的結論是有點距離的火堆事好的，但靠太近就不好了，這樣也就能讓他能獲得的分數最大化
 
7.
接下來是怎麼去知道我們處理的任務是回歸問題還是分類問題，一個簡單的區分方式就是假如我們希望模型給我們的輸出事類別資料也就是離散型的數據，像是剛剛分貓狗的問題異樣，這就是分類問題，而分類問題訓練模型的目的是找出一條決策邊界去良好的區分每一個類別，評估方式是用準確度之類的，這個我們有混淆矩陣去評估模型的好壞，這之後會介紹

相反的當我們希望模型的輸出是數值型的資料也就是連續的數據，像是購買金額這種，這樣就是回歸問題，而回歸模型的目的就是找出一條最優的回歸線去擬合所有資料，評估方式則是用誤差去判斷

這邊是意圖，模型的函數畫出來會長這樣

9.
首先先介紹線性回歸，圖片上是我們的資料集的散點圖，每個點都是一筆資料，x軸為學習時間，y軸為收入，如果現在給一個新的資料學習時間是10小時，要透過已知的資料去預測她的收入，這邊我們就可以使用線性回歸模型

線性回歸算是好理解而且用途又廣泛的一種算法，他像以前學過的二元一次方程式，當給定參數W0、W1時畫在座標圖內會是條直線，這也是線性的涵義

以剛剛的資料來說，當我們只用一個X來預測Y，也就是拿學習時間去預測收入，這就是最基本的一元線性回歸，而線性回歸模型最終的目的就是找出一條直線，並讓這條線盡可能地去擬合所有資料點

10.
在開始去找這條回歸直線前，我們先介紹機器學習的框架，基本上都是由模型、目標和算法組成的，對於一個數據集，我們首先需要根據數據的特點和目的來選擇適合的模型，以我們的例子來說的話，我們知道預測的輸出是收入，是連續型的數值，因此判斷是解回歸問題，而我們選擇了線性回歸來當作我們的模型，

有了模型之後，接下來要讓這些模型效能最佳化，這時候就需要給模型一個訓練的目標，而這個目標叫損失函數，通常我們會希望讓模型的損失函數最小化，

因此最後我們要選一個優化算法，去讓我們模型的參數在訓練得過長中去逼近我們的目標，讓損失函數最小化

基本的框架大概就是這樣

11.
這邊介紹損失函數，首先會用到殘差或是誤差這概念，也就是真實值和預測值間的差距，一樣剛剛的例子，紅點就是真實值，而紅點連線到預測函數上的點就是我們的預測值，他們之間的差距就誤差，

12.
知道誤差後，損失函數就算得出來了，在線性回歸模型中其中一種常用的算法就是加總所有誤差的平方值，後面會介紹更多的類別，

現在有了損失函數後，解下來就是如何用優化算法將他最小化

13.
這邊介紹兩個優化算法，第一個是最小二乘法或是叫最小平方法，通過最小化誤差的平方和來找最佳解，這邊我們用個例子說明

假如表格上是我們已知的資料，現在預測五支筆要多少錢，

14.
最一般的做法就是把線方程式列出來算，因此我們假設比的數量是X，Y是付款金額，B1 B2就是我們要計算出的未知參數，將這些條件帶進去就可以拿到四個方程式，但是很明顯沒有一組B1 B2是能完美符合四個方程式的，也就是說我們找不到一條直線能通過這四個資料點，像圖上這樣，因此雖然不能滿足找到一條能通過所有資料點德直線，我們還是會希望算出來的直線能盡可能地接近所有的資料點





15.
而最小二乘法去定義怎樣才算最接近所有的點的方式跟剛剛的損失函數長很像，就是將所以誤差的平方累加，就像上面的公是異樣，理想情況也就是完全沒誤差的話S=0，但如果沒辦法則是讓S逼近0


16.
而這裡去算S的最小值的方式是對參數B1 B2的求偏導數=0，導數就是自變量也就是X發生變化時，函數的值變化規律的描述方式，若有多個X，其中一個X的導數就會叫偏導，計算過程有興趣再了解就好
求完偏導=0我們得到了B1 B2分別是多少回歸函數也就算出來了

在剛剛的介紹中可以發現其實最小二乘法的步驟非常簡單，也就是將所有的資料點變成方程式去求偏導=0，但是這有一個問題，當我們的樣本和參數數量非常大時，計算會非常的困難，因此我們這邊就有了另一種優化算法，也就是梯度下降法

17.
梯度下降法的思想很好理解，假如我們現在在某個山的山頂上，現在我們要在天黑前到達山的最低點去進行裝備的補給，不需要考慮下山的安全性，在陡的懸癌下去也沒事，那怎麼下山最快?

最快的方法是以我們當前的位置為基準，尋找該位置最陡峭的方向，然後朝這個方向走去，走一段距離後，重新找最陡峭的方向，一直重複就可以達到最低點，這就是梯度下降在做的事，圖片中的山其實就是我們損失函數的解

而在下山的過程中會遇到兩個問題，第一個是如何測量陡峭的程度，第二個是我們每次走的距離要多長

18.
首先先說步長，步長有個專門的名字叫學習率，學習率若太小，我們到達山底的時間會很久，也就是模型迭代的次數會很多，而學習率太大則會讓我們在一開始下降很快，但到後期有很大的機率會直接跳過山底的那個點，
這邊是理想的圖

通常最簡單調學習率的方式是由大到小去調整，一開始我們需要用較大的找出一個正確的下降的方向，隨著訓練次數增加，模型參數更新的幅度會越來越小，因此我們通常會將我們的學習率降低訓練，降低的方式最基本的就是變十分之一，其他還有用指數下降的，這邊有興趣的可以去找最佳化理論





21.
這邊另外再跟大家介紹局部最佳解和全局最佳解，英文就是local minma和 global minma，這邊有找到gif跟大家解釋，紅點是每次更新找到的解，藍色是切線，紅色是髮線，全局最佳解就是最低點，而局部最佳解就是像圖上這種小凹槽的地方，當我們學習率太小或是初始直不好，就有可能掉到局部最小解出不來，而當我們學習率太大時，雖然能避開局部最小解，但能發現在接近最佳解附近時會因為步長太大碰步道最低點，因此會變成這樣反覆恆跳的樣子，而學習率剛好會長這樣

22.
理解下降的過程後，這邊是實際的算法，步驟也很少，首先會對損失函數或叫成本函數進行微分，也就是塗上cf的地方，再來就是乘以學習率alpha，最後拿上次更新的權重撿掉他，舊式更新後的權重了

23.
這邊介紹梯度下降的種類，批量梯度下降就是每次更新權重的時候，都會用所有的樣本在計算一次，這種方法的缺點就是資料量大時，每更新一次都會計算很久
24.
第二種則是隨機梯度下降，每次更新權重只用一個樣本來計算，缺點也很明顯，不好找到最低點
25.
因此這邊還有第三種方法，就是前兩種的折衷，每次使用的樣本數量在前兩者之間，取平衡
26.
到現在我們線性回歸計算過程就講完了，這邊介紹一下線性回歸有哪些類型，首先是簡單線性回歸，當我們只有一個特徵x和一個y得時候就叫簡單線性回歸

27.
當我們的特徵x不只一個時，就教多元線性回歸 

28.
最後是多項式回歸，前面兩個算法都需要我們的資料是線性分布的，而當我們要做回歸問題的資料呈現非線性時，我們就用多項式回歸處理









29.
接下來是模型擬合的問題，我們再做模型訓練的時候會把我們的資料切成訓練資料以及測試資料，在模型進行訓練的時候只拿訓練資料去使用，當我們訓練完後會拿測試資料去驗證結果，這邊的測試資料也就是模型沒看過的資料，

當我們模型的表現在訓練資料與測試資料上都不好的時候，就是低擬和underfit，這原因可能是我們參數選太少或是選錯模型去訓練
而當我們模型在訓練資料表現很好，但在測試資料表現很差的時候，就有可能是過擬和，我們的模型只能在已知的資料有好的結果但遇到新的資料就沒辦法去處理，這可能的原因有我們的模型複雜度太高，參數選太多之類的情況

30.
解決過擬合的方法常見的就是增加我們的資料量或是重新篩選特徵，還有另一個常用的就是正則化，正則化簡單來說就是在我們的損失函數裡加上一個懲罰項，就是公式右邊這東西，當我們加入越多的特徵的時候，也就是n月大的時候，我們的成法項也會變大，因此我們的損失函數就越難最小化，用這種方式來控制我的參數的使用量不要過多
常用的正則化分成，L1正則合L2正則兩種

31.
接下來是回歸模型常用的評估指標，像是有平均絕對誤差 均方誤差 均方根誤差 R平方 還有平均絕對百分比誤差之類的，大致上都是用我們預測結果減掉實際資料後再去做一些數學的轉換
https://blog.csdn.net/AIYA_aya/article/details/95905319

37.
接下來是分類模型，首先是邏輯回歸，邏輯回歸跟他的名字一樣雖然是做分類用的，但中間的計算過程依樣是做回歸，會這樣說的原因是邏輯回歸的計算結果是0到1之見的連續數值，這邊就是發生機率的意思，當我們加上一個玉值像是大於0.5就判對維A類小於0.5就判斷為B類後他就可以拿來做分類的








38.
接下來說明如邏輯回歸如何去將結果轉換為發生機率的，拿剛剛學習時間合收入的例子來解釋，這圖示剛剛現行回歸我們資料的樣子，但如果將這個問題變判斷學習時間是否會增加收入的分類問題的話，我們的資料分布會變成項這樣，結果只會有有增加或沒增加兩種，也就是0跟1，此時如果用剛剛的線性回歸模型計算的話，模型的函數化出來就會像塗上的紅線，可以發現如果用線性回歸的話無法良好的去擬和我們的資料點，因此這邊會用一個叫SIGMOID的函數去將我們的結果轉換為概率

39.
SIGMOID函數的是一個S型的曲線，透過將我們線性的值映射到SIGMOID函數上就會變成我們發聲的概率，SIGMOID函數也就是我們邏輯回歸模型去擬和資料點的方式

40.
而邏輯回歸的損失函數是用交叉商去計算，這邊只要知道意義就好，簡單來說商這東西代表的是對給定分布的不確定性的度量標準，而當我們預測結果與實際值月相同我們的商就越接近0，也就是不確定性越低，而如何去優化我們的交叉商，其實就跟前面的梯度下降方式異樣，只是我們換了一個損失函數，其他的計算就都異樣了


















42.
接下來是Knn k近鄰算法，這邊的內容我把它連同demo放在一起，所以用python的其中一個編譯器jupyter notebook來介紹


KNN.
KNN這算法算是一個比較簡單好懂的模型，它可以用來分類也可以用來預測數值

這次用的資料airbnb的房間價格的資料，然後我們要做的事就是去預測房間能租多少錢 


首先我們去讀取我們的資料，用之前提過的pandas套件讀取，然後因為原始的資料特徵還蠻多的，這邊只選用了部分特徵做訓練也就是features的部分，然後讀取csv並用選好的特徵去篩選資料，接著在將資料print出來，這邊說明一下選用的特徵，包含可容納的人數，臥室和廁所數量，床的數量，price是每晚的費用，也就是我們要預測的目標，另外還有其他客人最少租幾天最多租幾天跟評論數量

知道我們的資料有哪些後，現在假如我有一個有三個房間的房子，我能租多少錢，最簡單的方式就是看其他異樣是三個房間的房子都租多少錢再來決定，這邊也就是k近鄰的概念，近鄰的意思就是選擇與我們預測或分類的目標最相似的資料，而k的意思是當我們去找相似的資料時要找幾個相似的資料，找3個k就等於3，找5個k就等於5，以此類推，這邊大家應該就知趙k近鄰是甚麼了，當我現在有一筆資料，我們就拿這資料與整個資料及比較，看看有哪些資料與我們相似，然後找k個相似的資料去做計算











然後按照剛剛的思路，首先第一步拿到一筆有一間房間的資料，我們想知道價格能訂多少，然後我們還有原始的資料集，在原始資料中有一個房間的也有三個房間的，每筆資料都已經有它的價格，然後我們進行對比，
假設原始資料就這五筆，接著去找最相似的資料，假設k等於3已就是找3筆，另外這邊有個相似度的欄位，這邊相似度的表示方式是用我們的房間數量減去原始資料的房間數量，所以等於0的就是最相似的，接著我們拿找到的最相似的三筆資料的房子價格取平均，也就得到了我們的房子大概能值多少錢了，這樣就走完了k近鄰的流程了

接著是該如何去定義我們的近鄰也就是相似的資料，最常用的就是用歐式距離去衡量，這邊公式已經列出來了，假設我們的資料有很多特徵，q是我們的特徵，q1到qn就是我們有n個特徵，p1到pn就是另外一條資料的特徵，計算方式就是對應的特徵去相減然後取平方，並將所有的平方值加總後開根號，這就是歐式距離的算法，一般情況對於連續的資料，我們都可以用歐式距離去定義他們之間的相似度，

現在假設我們有三個房間，我們就從資料裡找異樣是三個房間得資料，也就是距離為0的，有461個，接著將這461筆資料的順序隨機排序，來確保資料的隨機性，最後我們取前五個去做平均得到88也就是我們預測房間大概的價格

預測結果出來後，我們要對她進行評估，這邊就是剛剛前面講的，我們會將原始資料集去做切割，分成訓練資料和測試資料兩部分，這邊我們將資料的前2792筆當作我們的訓練及，之後的當作測試及，然後這邊是將剛剛講的預測價格的方式包成一個函數，接著我們預測測試資料的價格，執行後就得到每筆測試資料預測的值了，有了預測結果後，用剛剛提過的軍方跟誤差做計算，這樣就得到了我們模型的評估得分了

隨機發揮
