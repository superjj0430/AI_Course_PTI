Agenda

今天的內容主要會講機器學習有哪些類別，分類問題回歸問題是什麼，還有一些比較經典的模型

4.
機器學習依照學習的方式可以分成監督式、非監督式和強化學習，而依照處理的問題類別可分為回歸問題和分類問題兩種

5.
這邊先講監督式與非監督式學習，當我們使用的訓練資料是有標記答案的時候，像是分類貓狗的圖片時，已經事先知道哪些是貓哪些是狗，我們會稱為監督式學習

非監督式則相反，我們訓練資料沒有標記答案只有特徵，如果用剛剛例子來說，模型只能在一群資料裡分出兩個群體，但無法得知哪個群體式貓還是狗

這邊是示意圖，監督式就像圖片上一樣，已經知道了他式圈還是差，而非間度式則是不知道

6.
接下來是強化學習，強化學習是讓我們的模型在環境裡學習，模型的每個行動會有各自對應的獎勵，模型會以獲得最大效益為目的去學習

這邊舉個例子，想像模型是個小朋友，現在他第一次看到火，然後他走到了火的旁邊感受到溫暖，因此認為火是好東西加一分，然後他伸手去摸火，被燙到很痛減一分，因此模型得出的結論是有點距離的火堆事好的，但靠太近就不好了，這樣也就能讓他能獲得的分數最大化
 
7.
接下來是怎麼去知道我們處理的任務是回歸問題還是分類問題，一個簡單的區分方式就是假如我們希望模型給我們的輸出事類別資料也就是離散型的數據，像是剛剛分貓狗的問題異樣，這就是分類問題，而分類問題訓練模型的目的是找出一條決策邊界去良好的區分每一個類別，評估方式是用準確度之類的，這個我們有混淆矩陣去評估模型的好壞，這之後會介紹

相反的當我們希望模型的輸出是數值型的資料也就是連續的數據，像是購買金額這種，這樣就是回歸問題，而回歸模型的目的就是找出一條最優的回歸線去擬合所有資料，評估方式則是用誤差去判斷

這邊是意圖，模型的函數畫出來會長這樣

9.
首先先介紹線性回歸，圖片上是我們的資料集的散點圖，每個點都是一筆資料，x軸為學習時間，y軸為收入，如果現在給一個新的資料學習時間是10小時，要透過已知的資料去預測她的收入，這邊我們就可以使用線性回歸模型

線性回歸算是好理解而且用途又廣泛的一種算法，他像以前學過的二元一次方程式，當給定參數W0、W1時畫在座標圖內會是條直線，這也是線性的涵義

以剛剛的資料來說，當我們只用一個X來預測Y，也就是拿學習時間去預測收入，這就是最基本的一元線性回歸，而線性回歸模型最終的目的就是找出一條直線，並讓這條線盡可能地去擬合所有資料點

10.
在開始去找這條回歸直線前，我們先介紹機器學習的框架，基本上都是由模型、目標和算法組成的，對於一個數據集，我們首先需要根據數據的特點和目的來選擇適合的模型，以我們的例子來說的話，我們知道預測的輸出是收入，是連續型的數值，因此判斷是解回歸問題，而我們選擇了線性回歸來當作我們的模型，

有了模型之後，接下來要讓這些模型效能最佳化，這時候就需要給模型一個訓練的目標，而這個目標叫損失函數，通常我們會希望讓模型的損失函數最小化，

因此最後我們要選一個優化算法，去讓我們模型的參數在訓練得過長中去逼近我們的目標，讓損失函數最小化

基本的框架大概就是這樣

11.
這邊介紹損失函數，首先會用到殘差或是誤差這概念，也就是真實值和預測值間的差距，一樣剛剛的例子，紅點就是真實值，而紅點連線到預測函數上的點就是我們的預測值，他們之間的差距就誤差，

12.
知道誤差後，損失函數就算得出來了，在線性回歸模型中其中一種常用的算法就是加總所有誤差的平方值，後面會介紹更多的類別，

現在有了損失函數後，解下來就是如何用優化算法將他最小化

13.
這邊介紹兩個優化算法，第一個是最小二乘法或是叫最小平方法，通過最小化誤差的平方和來找最佳解，這邊我們用個例子說明

假如表格上是我們已知的資料，現在預測五支筆要多少錢，

14.
最一般的做法就是把線方程式列出來算，因此我們假設比的數量是X，Y是付款金額，B1 B2就是我們要計算出的未知參數，將這些條件帶進去就可以拿到四個方程式，但是很明顯沒有一組B1 B2是能完美符合四個方程式的，也就是說我們找不到一條直線能通過這四個資料點，像圖上這樣，因此雖然不能滿足找到一條能通過所有資料點德直線，我們還是會希望算出來的直線能盡可能地接近所有的資料點





15.
而最小二乘法去定義怎樣才算最接近所有的點的方式跟剛剛的損失函數長很像，就是將所以誤差的平方累加，就像上面的公是異樣，理想情況也就是完全沒誤差的話S=0，但如果沒辦法則是讓S逼近0


16.
而這裡去算S的最小值的方式是對參數B1 B2的求偏導數=0，導數就是自變量也就是X發生變化時，函數的值變化規律的描述方式，若有多個X，其中一個X的導數就會叫偏導，計算過程有興趣再了解就好
求完偏導=0我們得到了B1 B2分別是多少回歸函數也就算出來了

在剛剛的介紹中可以發現其實最小二乘法的步驟非常簡單，也就是將所有的資料點變成方程式去求偏導=0，但是這有一個問題，當我們的樣本和參數數量非常大時，計算會非常的困難，因此我們這邊就有了另一種優化算法，也就是梯度下降法

17.
梯度下降法的思想很好理解，假如我們現在在某個山的山頂上，現在我們要在天黑前到達山的最低點去進行裝備的補給，不需要考慮下山的安全性，在陡的懸癌下去也沒事，那怎麼下山最快?

最快的方法是以我們當前的位置為基準，尋找該位置最陡峭的方向，然後朝這個方向走去，走一段距離後，重新找最陡峭的方向，一直重複就可以達到最低點，這就是梯度下降在做的事，圖片中的山其實就是我們損失函數的解

而在下山的過程中會遇到兩個問題，第一個是如何測量陡峭的程度，第二個是我們每次走的距離要多長

18.
首先先說步長，步長有個專門的名字叫學習率，學習率若太小，我們到達山底的時間會很久，也就是模型迭代的次數會很多，而學習率太大則會讓我們在一開始下降很快，但到後期有很大的機率會直接跳過山底的那個點，
這邊是理想的圖

通常最簡單調學習率的方式是由大到小去調整，一開始我們需要用較大的找出一個正確的下降的方向，隨著訓練次數增加，模型參數更新的幅度會越來越小，因此我們通常會將我們的學習率降低訓練，降低的方式最基本的就是變十分之一，其他還有用指數下降的，這邊有興趣的可以去找最佳化理論





21.
這邊另外再跟大家介紹局部最佳解和全局最佳解，英文就是local minma和 global minma，這邊有找到gif跟大家解釋，紅點是每次更新找到的解，藍色是切線，紅色是髮線，全局最佳解就是最低點，而局部最佳解就是像圖上這種小凹槽的地方，當我們學習率太小或是初始直不好，就有可能掉到局部最小解出不來，而當我們學習率太大時，雖然能避開局部最小解，但能發現在接近最佳解附近時會因為步長太大碰步道最低點，因此會變成這樣反覆恆跳的樣子，而學習率剛好會長這樣

22.
理解下降的過程後，這邊是實際的算法，步驟也很少，首先會對損失函數或叫成本函數進行微分，也就是塗上cf的地方，再來就是乘以學習率alpha，最後拿上次更新的權重撿掉他，舊式更新後的權重了

23.
這邊介紹梯度下降的種類，批量梯度下降就是每次更新權重的時候，都會用所有的樣本在計算一次，這種方法的缺點就是資料量大時，每更新一次都會計算很久
24.
第二種則是隨機梯度下降，每次更新權重只用一個樣本來計算，缺點也很明顯，不好找到最低點
25.
因此這邊還有第三種方法，就是前兩種的折衷，每次使用的樣本數量在前兩者之間，取平衡
26.
到現在我們線性回歸計算過程就講完了，這邊介紹一下線性回歸有哪些類型，首先是簡單線性回歸，當我們只有一個特徵x和一個y得時候就叫簡單線性回歸

27.
當我們的特徵x不只一個時，就教多元線性回歸 

28.
最後是多項式回歸，前面兩個算法都需要我們的資料是線性分布的，而當我們要做回歸問題的資料呈現非線性時，我們就用多項式回歸處理

29.
