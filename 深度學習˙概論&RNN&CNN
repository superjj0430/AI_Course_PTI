深度學習概論

4.
深度學習之前說過是機器學習裡的一個分支，而深度學習和神經網路的關係大概就是大部分常用的深度學習算法都是屬於神經網路算法，因此通常講到深度學習就會提到神經網路


5.
這邊說明深度學習和機器學習最大的差別，圖片上是監督式機器學習的架構圖，分為訓練和預測階段，其中要注意的一點是會作特徵萃取或叫特徵工程這件事


6.
而下圖式深度學習的架構圖，可以發現這邊就沒有做特徵萃取，因此和機器學習相比，深度學習不需要我們自己提取特徵，模型會自行對數據進行篩選 


7.
接下來是一些神經網路的基礎，這邊會先將整個神經網路拆成幾個小部份去做說明，最後在合併起來看整個架構時，就會比較好去理解了

首先是線性函數或叫得分函數，假如現在做圖片分類的任務，輸入一隻貓，接下來要得到他在每個類別的一個得分，比如說屬於狗的得分屬於貓的得分屬於牛的得分

接著把這張圖在分細一點，這張圖總共有有32*32*3 3072個像速點，每個像速點對結果的影響的重要性也不太相同，比如分辨一隻貓，貓眼睛的資料和後面背景的資料重要性部會相同，而這邊所說的重要性也就是權重參數W，這邊有3072個像速點，每個像速點都會對應一個權重值，也就是會有3072個權重


8.
接著看算式，首先看藍字輸入X，這邊我們將貓的每個像速點拉長，因此會產生一個3072*1的矩陣，然後還有剛剛提到的權重參數，這裡有3072個像速點，權重矩陣就會變成1*3072的矩陣，接著輸入矩陣與權重矩陣相成，依照矩陣乘法的規則最終會得到一個1*1的矩陣也就是一個得分值，

這邊假設我們要做一個十分類的任務，也就是要判斷圖片是屬於十個類別中哪一個類別，對於每個像速點我們都要去計算他在這十個類別中各自的得分，最後在去評估結果，而對於權重W來說，有十個類別代表會有十組不同的W，也就是貓的類別有自己的權重狗的類別也有自己的權重，

因此這邊看到上面的這個紅字算式，3072代表的是每個類別有3072個權重值，而10代表的是有十個類別，然後綠色的部分因為有十個類別，我們最終獲得的值也會有十個，也就是這張圖片在這十個類別中各自的得分，這邊就是得分函數在數學上的表示，

最後看到紫色的地方叫做偏置項b，在神經網路中權重w通常是用來對結果有一個決定性的作用，而偏置項b則是做一個微調的動作，微調的意義是讓我們的神經網路在擬合上更加的靈活，而偏置項10*1的意思是每個類別都會有各自不同的偏置項，因此這邊有十個類別就會有十個偏置的意思

9.
然後這邊就是剛剛的計算方式，現在假設只將這張圖片分成只有四個像素點燃後不考慮它圖片是彩色還黑白的，像速點就是56、231這排，然後做一個三分類任務，類別分別是貓狗和船，對於這三個類別來說就會有三組權重W，這邊權重矩陣是一個3*4的矩陣，3代表有三個類別四代表有四個像速點，計算方式就是權重矩陣與輸入矩陣相乘，最後再加上偏置項B，就會得到這張圖片在每個類別的得分值了，然後這邊看一下權重矩陣裡面的值，可以發現數值有大有小，而這邊的大小也就是不同像速點對於分類類別的重要性，重要性越大值會越大，
然後說一下權重值是哪來的，通常初始值都是隨機值，在計算的過程中會透過梯度下降的方式去做優化，這邊可以看到分類結果的部分將貓分成了狗，也就代表這個神經網路還需要繼續優化


10.
然後是決策邊界，這邊簡單了解一下就可以了，權重參數是主要控制決策邊界的走勢，偏置參數是用來上下左右微調的，這邊大概看一下決策邊界會長怎樣就行了




11.
接著是如何衡量結果，跟機器學習異樣都會是用損失函數去衡量的，這邊損失函數複習一下，損失函數簡單來說就是數據的損失也就是誤差，加上一個懲罰項防止過擬合，最後一句話跟大家說明一下什麼意思，神經網路的優點是能解決的問題比傳統機器學習多，但他也有一個很大的缺點就是神經網路的效能過於強大，越強大的模型或算法他過擬合的風險就越高，所以說在實際在訓練神經網路的時候，我們很常不是因為他效能不好去修改他，而是因為效能太好才去修改他，因為一個過擬合的模型是不能拿來使用的

另外神經網路不管是分類任務還是回歸任務都是可以處理的，差別在於損失函數如何定義，神經網路的架構通常部會取改變，損失函數不同可以處理的任務也不同，


12.
接下來是SOFTMAX函數，這邊簡單回想一下剛剛的分類任務，我們神經網路計算出的結果都是一個得分值，但在實際上我們比較常會說的是這張圖片是貓的概率是多少是狗的概率是多少，這邊就跟我們之前說邏輯回歸模型很像，邏輯回歸是透過SIGMOID函數將分類結果映射成概率，而SOFTMAX的功能也是差不多的意思


13.
到這邊簡單REVIEW一下剛剛講的東西，首先會有一個藍色框的得分函數，輸入X配上權重W會得到一個得分值，有了得分值後，可以基於得分值和真實值去計算紅色框的損失函數，在計算完損失函數後加上綠色框的懲罰像會得到損失值，然後如果是分類任務會多一個SOFTMAX去轉換得分值變成概率，回歸任務的化部用轉換，到計算完損失值這步驟為止就叫神經網路的向前傳播，

而到這個步驟後接下來要做什麼，剛剛有說道神經網路會去更新權重，也就是找到最適合的權重，所以接下來要做的就是去優化權重降低我們的損失值，在神經網路裡這部分叫做反向傳播

這邊再跟大家說明一個點，剛剛用到的例子都是為了方便理解簡化過的，像是一個輸入X經過一組權重W計算就得到結果，但在實際上，權重部會只有一組，輸入X可能會經過很多組不同權重的計算才會得出一個結果值，這邊多組不同銓重跟前面時分類每個類別一組權重的意思不太異樣，這邊指的是光是一個類別裡就會有屬於該類別的好幾組權重，這點也是跟傳統機器學習不太異樣的地方，神經網路在得分函數計算的部分複雜化了很多，將原本只有一組的權重變成多組去計算


14.
接下來是是梯度下降，神經網路的反向傳播也是用梯度下降的方式去做計算，這邊之前有講過這邊就不再重複，投影片上的內容大家就當作複習快速看一下就可以了 


15.
這邊是梯度下降的三種類別


16. 
然後這邊講一個反向傳播的小例子，假設現在有XYZ三個輸入，最終結果會是FXYZ函數，他等於X+Y成Z，這邊FXYZ函數大家可以當作損失函數去看，現在我們將FXYZ函數拆開來，F可以看成Q成上Z籃框這便，而Q又等於X加上Y宏框這邊，接下來就是之前說過的透過求偏導找最小值，這邊分別要求XYZ三者的偏導，計算的順序從後往前算，首先先求F對Q跟Z的偏導，接著再求Q對XY的偏導，這邊也就說明神經網路在反向傳播的過程中，是由後往前逐層去計算的


17.
這種傳播方式有個名字叫做練式法則，這邊意思就是梯度在神經網路裡面傳遞的方式是一步一步船的，旁邊圖片就是剛剛計算過程的是意圖


18.
到這邊神經網路比較重要的細節大概都講過了，也就是向前傳播和反向傳播，接下來就是把剛剛講的東西合併起來去了解，組成一個完整的神經網路結構，

這邊看圖，左邊的是生物的神經網路，右邊是一個數學公式版的神經網路，這邊就是給大家看一下神經網路為什麼叫神經網路，因為他的出發點就是去模擬生物體內資訊透過神經網路的傳遞方式


19.
然這邊是神經網路的架構圖，這邊跟大家解釋神經網路是怎麼去工作的，首先是他有一個層次的結構，圖上可以看到他有個輸入層、有一個隱藏層1、隱藏層2和一個輸出層，這邊其實就是前面講的，輸入可能會經過多次轉換才會得到結果，也就是一層接著一層計算下去的，這就是層次結構

然後神經元，簡單來說就是我們的參數，向前面的例子，我們輸入3072個像速點，也就是在輸入層這邊我們會有3072個神經元

然後看到隱藏層1的地方，可以看到輸入層和隱藏層1的地方連接許多線，輸入層的每個神經元都和隱藏層1的美個神經元連接，這個結構就叫全連接，

而隱藏層中間四個神經元給大家說明一下，簡單來說就是神經網路為了能更好的去學習資料而將原始的特徵數量變多，他的做法可能是特徵1成0.1特徵2成0.2特徵3成0.3相加來的，而這特徵對神經網路來說可能比原始特徵更有意義，但對人類來說就沒辦法去對這些特徵去做一個具體的解釋，這也就是大家詬病的神經網路是黑盒子問題，神經網路這些特徵怎麼來的不知道、代表的意義也不知道，單純就是數學計算轉來的東西，

接著回來說明這些線是什麼，這先線其實就是一直提到的全重，也就是資料會經過各種全重計算去進行一系列的轉換，
然後最後一點非線性，在每個矩陣計算結束後要傳到下一層時，都會先經過一個非線性函數去轉換


20.
整體的計算方是就像這樣，這邊非線性函數用MAX函數來指定，MAX函數裡可以看到全重成上輸入，經過MAX的非線性轉換後才會成上下一個全重W2，如果在堆疊一層就會長的向下面異樣，

總結來說，神經網路為什麼這麼強大，也就是透過一堆網路層和各種數學計算去堆疊他的參數輛，通常參數的數量都會是百萬級別以上的，

但參數的數量不是說越多越好，通常參數或者說神經元越多，在訓練及的表現都會越好，但同時也越容易過擬合，而且參數越多也代表計算量越大，因此模型執行的速度也就會越久

另外就是當我們每增加一個神經元，對神經網路來說她不只是增加一個而已，而是增加一組神經元，因為美個神經元都會經過一系列的計算，因此假如我們增加一個神經元，他最終增加的參數數量可能是多了一百萬個這麼多，這也是神經網路的一個特色


21.
接下來看正則化也就是懲罰項對結果的影響，懲罰項越小越符合訓練及的結果，圖片上可以看到懲罰值越小越能擬和訓練及，但在測試集也就是實際結果上的表現則不意定會更好，也就是過擬合的問題


22.
然後是參數數量對結果的影響，參數越多越能擬和訓練及，但同樣在實際結果上的表現則不意定會更好


23.
接下來是激活函數，也就是前面提到的非線性轉換，透過加入非線性的因素，神經網路能更容易地去擬合不同形狀的資料，另外就是通常非線性轉換在每次做矩陣運算後都會執行一次，才能維持他非線性的結果

24.
這邊就舉三個比較常用的，首先是之前提過的SIGMOID，轉換完後在01之間，優點是算法比較簡單，缺點就是要算很久可能改變數居分布，還有一個神經網路裡很常會聽到的詞梯度消失，簡單來說就是當我們的參數經過轉換後越接近於0，他之後向後傳播的力度會越來越小，到最後會小到無法更新參數的問題，

跟他相對的是梯度爆炸，也就是參數越更新越大，大到神經網路無法計算的程度


25.
在來是TANH函數，轉換後在-11之間，優點是比起SIGMOID部會去改變數據分布，缺點是計算方式是冪運算，異樣不好算，而且還是有梯度消失的問題


26.
最後是RELU，這個算是裡面最常用的激活函數，他也可以叫線醒整流函數，特點就是運算速度很快，沒有梯度消失問題，但因為取值大小無上限而可能有梯度爆炸的問題，另外還有一個RELU DEAD問題，意思是當RELU轉換的值小於0，他會直接將他變成0，會導致梯度值皆消失無法反向傳播

27.
然後是資料前處理，這邊之前的課也講過了，標準化跟歸一化，這邊也不多做說明


28.
然後是參數初始化，通常一開始都是用隨機的給他選，這邊有RADOM前成一個0.01的意思是讓隨機選的參數差一步要太大，大概就這樣


29.
然後最後一部份，DROPOUT機制，神經網路最大的問題就是太複雜太容易過擬合，前面說可以加正則化處理，另一種方法就是用DROPOUT處理，透過削減神經網路網路層的威力來處理，左邊可以看到是原本的架構，可以發現全連結，美個神經元都相連著，而右邊的圖可以發現線就沒這麼多了，感覺稀疏了一點，右邊就是DROPOUT的結果，它的作用是在訓練的過程中，在每一層隨機殺死一些神經元，殺死的意思就是在這次的反向傳播更新全重的時候，不更新到殺死的神經元，以降低神經網路擬合的能力

到這邊基礎就講完了，接著介紹一些經典的模型
 
CNN

31.
CNN全名是CONVOLUTION NEYRAL NETWORK卷積神經網路，他通常是拿來處理圖片資料，在CNN出來前傳統神經網路處理圖片資料有個問題，就是算到後面參數會太多，這就導致過擬合跟電腦根本算不出來等問題，
下面例子可以看到假如輸入是100*100*3的圖片，第一層神經元有1000個，這樣第一層算完後得到的參數亮就是3*10的7次方，如果再多做幾層餐數量會直線上升

因此CNN透過他一些結構的設計避免掉參數過多的問題，並在2012年的IMAGENET競賽拿下冠軍，ALEXNET是模型架構的名稱，當年準確率比第二名整整多出11%，因此從那時候開始CNN就變成圖像處理的主流之一


32.
接著就是我們CNN最基本的結構，依序會介紹四種不同的神經網路層，有輸入資料的輸入層，CNN的核心卷積層，對數據降維採樣的池化曾，和將學習到的特徵轉換成分類結果的全連接層


33.
首先是輸入層，跟名稱異樣他表示的是整個神經網路的輸入，如果是圖片資料作為輸入，大小通常是w成h成1或三，這三個維度分別代表寬度長度和深度，其中深度又叫通道，在彩色圖中有rgb三個通道，而如果是黑白圖片則只有一個通道


34.
接下來卷積層，這是CNN的核心，首先獎卷積操作，卷積操作是透過使用FILTER或較卷積核，將當前網路上的紫節點矩陣轉換為下一層網路的節點矩陣，得到的下一層網路的節點矩陣又叫特徵圖，這邊直接看例子






35.
這邊我們假設圖片是灰階，也就是通道為1，然後我們再定義一個3成3的FILTER，FILTER的值其實就是權重，這邊數字是我自己定義的，但實際我們再訓練得時候都是用隨機初始化的方式

接下來進行卷積，卷積操作就是FILTER與FILTER覆蓋的局部區域矩陣對應的每個元素相乘後累加，

像圖片上的4就像塗上這樣算出來的，每個對應的點香橙後加總，這就是卷積操作


36.
接著，完成上面的卷積後，FILTER會繼續移動，然後在進行卷積，每次移動的距離叫做步長，這邊設定為1，可以看到塗上我們的FILTER往右移易格，然後會再做一次卷積，


37.
這邊是完整FILTER跑的過程，可以發現跑道最右邊後會自動往下一格，然後繼續從左到右跑，直到整張圖都被FILTER覆蓋一遍，最終右邊的矩陣就是輸出也就是特徵圖


38.
接著，為什麼用FILTER可以提取特徵，首先如果與FILTER越相似，卷積後輸出的執會越大，職越大也就代表特徵的重要性越大

再來，FILTER只覆蓋局部區域，這樣可以讓特徵可以專注在各自負責的部分

最後就是全值共享，可以發現在剛剛卷積的過程中，我們FILTER裡的權重不管對應到圖片的哪個區域，裡面的值都是不改變的，也就是所有資料都共享這個FILTER的權重，這樣可以降低過擬合的機會





39.
多通道輸入，前面内容将输入层简化为单通道结构。在实际应用中，输入层通常通常包含多个矩阵的叠加，也就是多通道结构
比如，一张彩色图像包含3通道结构。这种情形下，filter的深度由输入矩阵的深度决定。上图中，输入矩阵包含了三个通道的数据，那么filter也会包含三个矩阵。卷积计算过程与之前不同的地方在于，每一层的filter在与对应层的输入矩阵进行卷积后，会接着再把各个层计算的结果相加得到输出值


40.
接下來激活函數，CNN最常用的是剛剛提過的RELU，把卷積層的輸出做非線性的映射，卷積的輸出值若小於0，經過映射後就會變成0


41.
這邊力圖，0.77大於0，經過RELU映射後異樣0.77，但第二個-0.11小於0，經過RELU後會直接等於0，這邊是整張圖RELU完後的結果，可以發現小於0得部分都被轉換為0了


42.
接下來是多個FILTER操作，一個FILTER是對應一個特徵，因此通常會有多個FILTER去提取圖片的多個特徵，跟多組權重計算的意思是異樣的，這邊假設我們用兩個FILTER，這邊出來的兩張特徵圖會重新組合成深度為2的圖片，作為下一層卷積的輸入


43.
解下來是PADDING，PADDING是在輸入矩陣的周圍進行數據填充，通常用0來填充，像PADDING為1時就填充一圈0








44.
而PADDING的主要功用有兩個，圖片在經過卷積後，大小會越來越小，當我們有多個卷積層的話，圖片會變小的特別明顯，因此可以用PADDING維持圖片大小

再來是在剛剛FILTER在圖片上移動的過程可以發現，圖片越靠中心的部分被FILTER掃描到的次數越多，因此透過PADDIN可以將原本在邊緣的資料往中間集中，可以減少資料處理不平衡的問題


45.
接下來池化曾，主要作用就是壓縮我們的特徵圖，保留有用的信息，池化主要分兩種，MAX POOLING就是取最大值最為壓縮後的數值，AVERAGE就是取平均值


46.
全連結層簡單來說就是要對結果做分類，对計算得到的特征图进行维度上的改变，来得到每个分类类别对应的概率值。


47.
這邊也舉個例子，原图片尺寸为9X9，在一系列的卷积、relu、池化操作后，得到尺寸被压缩为2X2的三张特征图
得到了2X2的特征图后，对其应用全连接网络，並且透過Softmax输出的是每个对应类别的概率值

接著将三个特征图改变维度直接变成一维的数据，展开的数据即为属于类别X的概率值，值大小也在对应X的线条粗细中表现出来，結果很明顯判斷圖片是叉

假如現在有張不那麼標準的圖片

进行一系列操作后，0.92表示极其大可能是X，因此对应到X的黄色线条比对应到O的绿色线条要粗很多很多，因此可判断这张图片為叉



48.
最後總結一夏，cnn主要由两部分组成，一部分是特征提取（卷积、激活函数、池化），另一部分是分类识别（全连接）

假設我們要分辨一隻貓的時候，透過特征提取（卷积、激活函数、池化）找出各部位特徵，最後再將特徵透過全連接合併得到一個結果
 
RNN

50.
接下來是循環神經網路RNN，這邊講義下為什麼他會出現，傳統的神經網路的輸出與輸入都是互相獨立的，以圖片辨識來說，我每一張圖片通過模型得到的結果，與其他輸入資料無關，我丟進去一張貓的圖片，其他憶起丟進去的圖片不管是貓是狗是牛，都部會影響我的輸出，
但假如我們現在要處理的任務，他每個輸入都與其他輸入有一些關西，那用傳統神經網路的效果就部會那麼理想，最常見的就是和語言相關的任務，下面有個簡單的例子，NLP中文叫自然語言處理，也就是比較常聽到的文字探勘，現在我們要去辨識這兩句話中KEEP代表的意思，可以發現雖然兩句話的KEEP寫法都異樣，但根據其他單詞的詞性，造成一個代表動詞的存放，一個代表名詞的生活費，這也就是剛剛所說的，當我們輸入與其他輸入有關時，我們用傳統神經網路將牠們分開來獨力辨識會有無法判斷的問題


51.
這邊是RNN架構圖，架構部會很複雜，而讓RNN能處理序列資料的主要差異就是他加入了時間順序的概念，圖上的輸入X與輸出H的旁邊都有寫個T，T代表當前的時間，T-1代表前一個T+1代表下一個


52.
這邊再將這個結構化簡來看，這邊先把旁邊的W遮住，可以發現RNN也是由輸入層、隱藏層和輸出層構成的，像X代表輸入，經過權重U的計算到達隱藏層S，再經過一次計算變成輸出，


53.
現在來了解意下剛剛遮住的W是甚麼，W叫做循環層，剛剛有說道RNN的每個輸出不完全取決於當前輸入，而是與其他輸入有關，這邊我們將左邊的圖案照時間線展開就比較清楚了，當我們的網路在T時刻接收到輸入XT時，隱藏層的值是ST，輸出值是OT，關鍵的一點是ST的值不僅僅取決於XT，還取決於ST-1，簡單來講就是我們在計算當前T時刻的輸出時，會用到當前ST的參數與前一時刻ST-1的參數，因此RNN就可以用這種方式將不同時間的輸入產生關連，上面是計算公式，也可以看到計算當前ST時有用到ST-1


54.
接下來有一個動圖幫大家更好理解RNN執行的時候是怎樣工作的，首先是有個當前輸入XT，和前一個隱藏層參數HT-1，接著會把隱藏層參數HT-1與HT合併計算，計算完後會產生一個輸出，並且模型會把當前隱藏層參數傳遞到下一個模塊繼續進行計算，整體RNN的理論大概就是這樣


LSTM

56.
接下來是RNN的其中一種變形LSTM，剛剛在介紹RNN架構時可以發現，他所使用到的隱藏層資料只有當前輸入的前一個隱藏層，當我們資料很長一串時會產生一個問題，當我們的隱藏層參數一直不段傳遞下去時，越前面的參數所造成的影響會越來越小，也就是模型把前面的參數弄丟的感覺，在學術上叫做長距離依賴問題，而LSTM主要就是要解決這事情，他跟RNN比起來多了一些不同的東西


57.
這邊是LSTM的架構，，可以發現LSTM和RNN異樣神经网络结构都是由完全相同结构的模块进行复制而成的，但他比RNN多了一些不同的神經網路層，並且透過一些簡單的數學計算來解決長距離依賴的問題，

在將她拆開來說明前先說明一下圖中所使用的圖標，綠色大框代表細胞狀態，黃色方塊是神經網路層，粉紅原點是數學運算，黑色箭頭代表向量的傳遞友就是參數，合併的箭頭代表參數的合併，分開代表複製

58.
首先是單元狀態或叫細胞狀態，他試著一條鏈運行的，lstm透過增刪單元中的訊息來維持長期資訊


59.
遺忘門，跟名字異樣他就是要用來忘掉某些東西，当输入新的信息时，模型若需遗忘旧的信息，此时通过遗忘门来完成。遗忘门是LSTM单元的关键组成部分，主要是用來决定LSTM从上一时刻的细胞状态 Ct-1 中丢弃什么信息。
，并且避免梯度随时间反向传播时引发的梯度消失和梯度爆炸的问题。

60.
遺忘們门读取 前一層隱藏層參數ht-1 和 輸入xt ，然后通过sigmoid将其映射到0到1之间的数值，最终该数值再与细胞状态Ct-1相乘，来决定Ct-1中该丢弃什么信息。当该数值为1时表示完全地保留 Ct-1的信息，当该数值为0时表示完全地丢弃 Ct-1 的信息


61.
接著是輸入門，功能是确定哪些新的信息被保留在细胞状态中ct中，
输入门用于控制网络当前输入数据xt 流入记忆单元的多少，即有多少输入信息可以保存到 ct 中。


62.
输入门包括两部分，第一部分：由sigmoid组成的“输入门”产生的介于0到1之间的控制信号 it ，用来控制 ct加帽子 输入的程度；第二部分：通过一个tanh层产生当前时刻的候选细胞状态 ct加帽子，这个值将由 it 决定添加到细胞状态中的程度


63.
接著就是要更新當前細胞狀態，将旧的细胞状态 ct-1 更新为当前细胞状态 ct 。有了遗忘门产生的控制信号ft ，tanh层产生的候选细胞状态 ct加帽子 ，输入门产生的控制信号 it ，就可以将 ct-1更新为 ct 。首先 ft乘上ct-1 确定上一个细胞状态要保留的信息；然后it乘上ct加帽子得到候选细胞状态需要保留的信息；最后将这两部分相加，得到最终的当前时刻的细胞状态ct，到這邊就完成細胞狀態的更新了








64.
最後就是輸出門的結構，输出值基于细胞状态，但是会有一个过滤的过程，

65.
这里也包括两部分操作：第一部分，由sigmoid组成的“输出门”产生的介于0到1之间的控制信号 ot ；第二部分，将最终产生的输出信息 經過tanh計算的ct 与控制信号 ot相乘，得到最终的输出值 ht 。输出门控制记忆单元ct 对当前输出值 ht 的影响，也就是CT中的哪一部分会在时间T下的 输出

最後和RNN異樣會將資料傳遞到下一個時刻T+1，除了将包含长期信息的细胞状态 CT 传递到下一时刻，也将当前时刻的输出HT作为近期信息传递到下一时刻，到這邊LSTM的工作原理就講完了，後面異樣有一個執行過程的動圖


66.
首先異樣有個當前輸入XT，和前一個隱藏層參數HT-1，接著是遺忘門的計算後更新細胞狀態CT-1，接著是輸入們的計算將CT-1更新乘CT，然後透過輸出們計算決定CT中那些資料會繼續傳遞下去，接著將細胞狀態CT和隱藏層參數HT傳遞到下一個LSTM模塊進行計算

到這邊深度學習相關的基礎和一些常見的模型就說完了
